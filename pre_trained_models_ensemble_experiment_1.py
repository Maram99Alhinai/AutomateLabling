# -*- coding: utf-8 -*-
"""pre_trained_models_ensemble_experiment_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Jv8k_Z1vxZH9-gf_U9bGMUQF-bytrOXS
"""

from PIL import Image

pip install transformers

"""# Scrap images"""

#source : https://github.com/R-K-H/vogue-runway-scraper

img_path = '/content/drive/MyDrive/IndustrialML/chanel/chanel_0.jpg'
raw_image = Image.open(img_path).convert('RGB')

import plotly.express as px
px.imshow(raw_image)

"""## BLIP (image to text) -> generate basic/general description

source: https://huggingface.co/Salesforce/blip-image-captioning-large
"""

import requests
from PIL import Image
from transformers import BlipProcessor, BlipForConditionalGeneration

processor_blip = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-large")
model_blip = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-large").to("cuda")

"""notice two approaches - conditional and unconditional caption generation"""

answers_template = [
"The overall style or aesthetic of the clothing in the picture is ",
"The dominant colors in the outfits are ",
"Type of occasion or event these clothes be suitable for is ",
"Personality or vibe that these clothes convey is ",
];

# conditional image captioning
text = "a photography of"
inputs = processor_blip(raw_image, text, return_tensors="pt").to("cuda")

out = model_blip.generate(**inputs)
print(processor_blip.decode(out[0], skip_special_tokens=True))

# unconditional image captioning
inputs = processor_blip(raw_image, return_tensors="pt").to("cuda")

out = model_blip.generate(**inputs)
print(processor_blip.decode(out[0], skip_special_tokens=True))

# conditional image captioning
for id, item in enumerate(answers_template):
  inputs = processor_blip(raw_image, item, return_tensors="pt").to("cuda")

  out = model_blip.generate(**inputs)
  print(processor_blip.decode(out[0], skip_special_tokens=True))

  if id == 0:
    # unconditional image captioning
    inputs = processor_blip(raw_image, return_tensors="pt").to("cuda")

    out = model_blip.generate(**inputs)
    print(processor_blip.decode(out[0], skip_special_tokens=True))

"""**Conclusion**

The model can be used for generating very general description

##Add evaluation

####ROUGE-L Score
"""

import nltk
nltk.download('all')

import pandas as pd
import nltk
from nltk.translate.bleu_score import corpus_bleu, sentence_bleu
from nltk.translate.bleu_score import SmoothingFunction

# Download required nltk data if not already available
nltk.download("punkt")

#open dataset.csv
dateset_path='/content/drive/MyDrive/IndustrialML/chanel/dataset.csv'
df=pd.read_csv(dateset_path)

# ROUGE-L evaluation function
def rouge_l_evaluation(reference, candidate):
    scorer = nltk.translate.bleu_score.SmoothingFunction().method4
    return sentence_bleu(reference, candidate, smoothing_function=scorer)


print(df.iloc[0,2])
# Perform caption generation and ROUGE-L evaluation
for item in answers_template:
    inputs = processor_blip(raw_image, item, return_tensors="pt").to("cuda")
    out = model_blip.generate(**inputs)
    generated_caption = processor_blip.decode(out[0], skip_special_tokens=True)

    # Compute ROUGE-L score for the generated caption compared to dataset captions
    rouge_l_score = rouge_l_evaluation(df.iloc[0,2], generated_caption)
    print("Generated Caption: ", generated_caption)
    print("ROUGE-L Score: ", rouge_l_score)

"""####BERT embeddings"""

from transformers import BlipProcessor, BlipForConditionalGeneration, BertTokenizer, BertModel
import torch

# Initialize BERT tokenizer and model
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model_bert = BertModel.from_pretrained("bert-base-uncased").to("cuda")

# Semantic similarity evaluation function using BERT embeddings
def semantic_similarity_evaluation(reference, candidate):
    reference_tokens = tokenizer(reference, return_tensors="pt", padding=True, truncation=True).to("cuda")
    candidate_tokens = tokenizer(candidate, return_tensors="pt", padding=True, truncation=True).to("cuda")

    with torch.no_grad():
        reference_embeddings = model_bert(**reference_tokens).last_hidden_state.mean(dim=1)
        candidate_embedding = model_bert(**candidate_tokens).last_hidden_state.mean(dim=1)

    similarity_scores = torch.cosine_similarity(reference_embeddings, candidate_embedding)
    return similarity_scores.mean().item()

# Perform caption generation and semantic similarity evaluation
for item in answers_template:
    inputs = processor_blip(raw_image, item, return_tensors="pt").to("cuda")
    out = model_blip.generate(**inputs)
    generated_caption = processor_blip.decode(out[0], skip_special_tokens=True)

    # Compute semantic similarity score for the generated sentence compared to dataset paragraphs
    semantic_similarity_scores = [semantic_similarity_evaluation(paragraph, generated_caption) for paragraph in df.iloc[0,2]]
    average_semantic_similarity_score = sum(semantic_similarity_scores) / len(semantic_similarity_scores)

    print("Generated Caption: ", generated_caption)
    print("Semantic Similarity Score: ", average_semantic_similarity_score)

"""Generated Caption:  the overall style or aesthetic of the clothing in the picture is a bit of a woman's
Semantic Similarity Score:  0.31138405182674056
Generated Caption:  the dominant colors in the outfits are the same, but the coat is different
Semantic Similarity Score:  0.28745444451080937
Generated Caption:  type of occasion or event these clothes be suitable for is a long coat
Semantic Similarity Score:  0.30072453241167163
Generated Caption:  personality or vibe that these clothes convey is a lot of attention
Semantic Similarity Score:  0.32009085868555237


Generated Caption:  the overall style or aesthetic of the clothing in the picture is a bit of a woman's
Semantic Similarity Score:  0.3101367787677522
Generated Caption:  the dominant colors in the outfits are the same, but the coat is different
Semantic Similarity Score:  0.2861041747250313
Generated Caption:  type of occasion or event these clothes be suitable for is a long coat
Semantic Similarity Score:  0.29994302911827775
Generated Caption:  personality or vibe that these clothes convey is a lot of attention
Semantic Similarity Score:  0.31861374765604644

# Vision Question Answering (Generate answers to a list of queries)
"""

# source: https://huggingface.co/dandelin/vilt-b32-finetuned-vqa

queries = [
"Can you describe the overall style and aesthetic of the clothing in the picture?",
"What are the dominant colors in the outfits?",
"What type of occasion or event would these clothes be suitable for?",
"Could you describe the personality or vibe that these clothes convey?"
];

answers_template = [
"The overall style or aesthetic of the clothing in the picture is ",
"The dominant colors in the outfits are ",
"Type of occasion or event these clothes be suitable for is ",
"Personality or vibe these clothes is ",
];

from transformers import ViltProcessor, ViltForQuestionAnswering
import requests
from PIL import Image

processor_vqa = ViltProcessor.from_pretrained("dandelin/vilt-b32-finetuned-vqa")
model_vqa = ViltForQuestionAnswering.from_pretrained("dandelin/vilt-b32-finetuned-vqa")

# prepare image + question
text = queries[3]
image = Image.open(img_path)

# prepare inputs
encoding = processor_vqa(image, text, return_tensors="pt")
# forward pass
outputs = model_vqa(**encoding)
logits = outputs.logits
idx = logits.argmax(-1).item()
print("Predicted answer:", model_vqa.config.id2label[idx])

"""let's get answers to all queries"""

answers = []

for i, query in enumerate(queries):
  # prepare inputs
  encoding = processor_vqa(image, query, return_tensors="pt")
  # forward pass
  outputs = model_vqa(**encoding)
  logits = outputs.logits
  idx = logits.argmax(-1).item()
  answers.append(answers_template[i]+model_vqa.config.id2label[idx])

print('\n'.join(answers))

"""**Conclusion**

The model can be used for very concrete and specific questions

# Zero-Shot Image Classification
"""

# source: https://huggingface.co/openai/clip-vit-large-patch14

queries = [
"Can you describe the overall style and aesthetic of the clothing in the picture?",
"What are the dominant colors in the outfits?",
"What type of occasion or event would these clothes be suitable for?",
"What are the key fashion trends or influences evident in these outfits?",
"Are there any particular body types or figures that these clothes would flatter?",
"Can you suggest the age group or demographic that these outfits would appeal to?",
"What are the notable designer brands or fashion houses associated with these garments?"
]

query_classes = [
    ['formal', 'cute', 'sexy', 'rock'],
    ['back', 'white', 'pink', 'green', 'grey'],
    ['date', 'party', 'casual', 'wedding', 'concert'],
    ['artdeco', 'popart', 'victorian'],
    ['slim', 'medium', 'large'],
    ['generation Z', 'millenia'],
    ['chanel', 'dior', 'noname']
]

from PIL import Image
import requests
from transformers import CLIPProcessor, CLIPModel

model_zhic = CLIPModel.from_pretrained("openai/clip-vit-large-patch14")
processor_zhic = CLIPProcessor.from_pretrained("openai/clip-vit-large-patch14")

for idx, query in enumerate(queries):
  classes = query_classes[idx]
  inputs = processor_zhic(text=classes, images=image, return_tensors="pt", padding=True)
  outputs = model_zhic(**inputs)
  logits_per_image = outputs.logits_per_image # this is the image-text similarity score
  probs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities

  max = logits_per_image.argmax(-1).item()
  print(classes[max])
  print(probs)

"""**Conclusion**

The model can be used for open questions, but the list of probable classes should be written manually (or using another model)

# Segment a photo
"""

from transformers import AutoFeatureExtractor, SegformerForSemanticSegmentation
from PIL import Image
import requests
import matplotlib.pyplot as plt
import torch.nn as nn
import os
import numpy as np
import torchvision.utils as vutils


extractor = AutoFeatureExtractor.from_pretrained("mattmdjaga/segformer_b2_clothes")
model = SegformerForSemanticSegmentation.from_pretrained("mattmdjaga/segformer_b2_clothes")

input_folder_path = '/content/drive/MyDrive/IndustrialML/chanel/'
output_folder_path = '/content/drive/MyDrive/IndustrialML/chanel_segmented/'

if not os.path.exists(output_folder_path):
    os.makedirs(output_folder_path)

# Iterate over the images in the input folder
for filename in os.listdir(input_folder_path):
    if filename.endswith('.jpg') or filename.endswith('.png'):  # Update with supported image extensions
        # Read the image
        image_path = os.path.join(input_folder_path, filename)
        image = Image.open(image_path)

        inputs = extractor(images=image, return_tensors="pt")
        outputs = model(**inputs)
        logits = outputs.logits.cpu()

        upsampled_logits = nn.functional.interpolate(
            logits,
            size=image.size[::-1],
            mode="bilinear",
            align_corners=False,
        )

        pred_seg = upsampled_logits.argmax(dim=1)[0]

        # Extract the upper cloth mask
        upper_cloth_label = 4  # Replace with the actual label index for the upper cloth
        upper_cloth_mask = pred_seg == upper_cloth_label

        # Convert the tensor to a NumPy array
        img_upper_cloth_mask = upper_cloth_mask.numpy().astype(np.uint8) * 255

        # Convert the NumPy array to a PIL Image
        image_pil = Image.fromarray(img_upper_cloth_mask)

        # Specify the output image file path with the .jpg extension
        output_image_path = os.path.join(output_folder_path, filename)
        output_image_path = output_image_path.replace('.png', '.jpg')

        # Save the image as JPEG
        image_pil.save(output_image_path)

import cv2
import os
import numpy as np

image_folder =  '/content/drive/MyDrive/IndustrialML/chanel/'
mask_folder =  '/content/drive/MyDrive/IndustrialML/chanel_segmented/'
output_folder_path =  '/content/drive/MyDrive/IndustrialML/chanel_final/'

if not os.path.exists(output_folder_path):
    os.makedirs(output_folder_path)

image_files = os.listdir(image_folder)

for image_file in image_files:
    # Load the image and mask
    image_path = os.path.join(image_folder, image_file)
    mask_path = os.path.join(mask_folder, image_file)  # Assuming mask filenames are the same as image filenames
    image = cv2.imread(image_path)
    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)

    # Apply the mask as a binary filter
    masked_image = cv2.bitwise_and(image, image, mask=mask)

    output_image_path = os.path.join(output_folder_path, image_file)
    output_image_path = output_image_path.replace('.png', '.jpg')

    if masked_image is None:
      print("Error: Failed to load the image.")
      continue

    # Save the masked image
    cv2.imwrite(output_image_path, masked_image)

"""# Build overall descriptions"""

import requests
from PIL import Image
from transformers import BlipProcessor, BlipForConditionalGeneration

processor_blip = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-large")
model_blip = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-large").to("cuda")

def get_basic_descr(processor_blip, model_blip, image):
  # conditional image captioning
  text = "a photography of"
  inputs = processor_blip(raw_image, text, return_tensors="pt").to("cuda")

  out = model_blip.generate(**inputs)
  descr = processor_blip.decode(out[0], skip_special_tokens=True)
  return descr

from transformers import ViltProcessor, ViltForQuestionAnswering
import requests
from PIL import Image

processor_vqa = ViltProcessor.from_pretrained("dandelin/vilt-b32-finetuned-vqa")
model_vqa = ViltForQuestionAnswering.from_pretrained("dandelin/vilt-b32-finetuned-vqa")

queries = [
"Can you describe the overall style and aesthetic of the clothing in the picture?",
"What are the dominant colors in the outfits?",
"What type of occasion or event would these clothes be suitable for?",
];

answers_template = [
"The overall style or aesthetic of the clothing in the picture is ",
"The dominant colors in the outfits are ",
"Type of occasion or event these clothes be suitable for is ",
];

def generate_close_questions_descr(processor_vqa, model_vqa, queries, answers_template):
  answers = []

  for i, query in enumerate(queries):
    # prepare inputs
    encoding = processor_vqa(image, query, return_tensors="pt")
    # forward pass
    outputs = model_vqa(**encoding)
    logits = outputs.logits
    idx = logits.argmax(-1).item()
    answers.append(answers_template[i]+model_vqa.config.id2label[idx])

  return '\n'.join(answers)

queries_zhic = [
"Could you describe the personality or vibe that these clothes convey?"
"What type of occasion or event would these clothes be suitable for?",
"What are the key fashion trends or influences evident in these outfits?",
"Are there any particular body types or figures that these clothes would flatter?",
"Can you suggest the age group or demographic that these outfits would appeal to?",
"What are the notable designer brands or fashion houses associated with these garments?"
]

answer_templates_zhic = [
"The most probable personality or vibe that these clothes convey is "
"The most probable type of occasion or event would these clothes be suitable for is ",
"The most probable key fashion trend or influence evident in these outfits is ",
"The most probable particular body types or figures that these clothes would flatter is ",
"The most probable age group or demographic that these outfits would appeal to is ",
"The most probable notable designer brands or fashion houses associated with these garments is "
]

query_classes = [
    ['formal', 'cute', 'sexy', 'rock'],
    ['date', 'party', 'casual', 'wedding', 'concert'],
    ['artdeco', 'popart', 'victorian'],
    ['slim', 'medium', 'large'],
    ['generation Z', 'millenia'],
    ['chanel', 'dior', 'noname']
]

from PIL import Image
import requests
from transformers import CLIPProcessor, CLIPModel

model_zhic = CLIPModel.from_pretrained("openai/clip-vit-large-patch14")
processor_zhic = CLIPProcessor.from_pretrained("openai/clip-vit-large-patch14")

def generate_open_questions_descr(processor_zhic, model_zhic, queries_zhic, answer_templates_zhic):
  answers = []
  for idx, query in enumerate(queries_zhic):
    classes = query_classes[idx]
    inputs = processor_zhic(text=classes, images=image, return_tensors="pt", padding=True)
    outputs = model_zhic(**inputs)
    logits_per_image = outputs.logits_per_image # this is the image-text similarity score
    probs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities

    max = logits_per_image.argmax(-1).item()
    answers.append(answer_templates_zhic[idx] + classes[max])
  return '\n'.join(answers)

segmented_image_path = '/content/drive/MyDrive/IndustrialML/chanel_final/chanel_0.jpg'
raw_image = Image.open(segmented_image_path).convert('RGB')

import plotly.express as px
px.imshow(raw_image)

def generate_description(image):
  description = ""
  description += get_basic_descr(processor_blip, model_blip, image)
  description += ' \n'+ generate_close_questions_descr(processor_vqa, model_vqa, queries, answers_template)
  description += ' \n'+ generate_open_questions_descr(processor_zhic, model_zhic, queries_zhic, answer_templates_zhic)
  return description

segmented_photo_description = generate_description(raw_image)
print(segmented_photo_description)

image_path = '/content/drive/MyDrive/IndustrialML/chanel/chanel_0.jpg'
raw_image = Image.open(image_path).convert('RGB')

import plotly.express as px
px.imshow(raw_image)

photo_description = generate_description(raw_image)
print(photo_description)

"""# Generate dataset"""

pip install openai

from google.colab import drive
drive.mount('/content/drive')

queries_gpt = [
  "Describe what type of clothes you see?"
  "Describe the overall style of the clothing in the picture?",
  "What are the dominant colors in the outfits?",
  "What type of occasion or event would these clothes be suitable for?",
  "Are there any specific details or embellishments on the garments that catch your attention?",
  "What are the key fashion trends or influences evident in these outfits?",
  "Are there any particular body types or figures that these clothes would flatter?",
  "Can you suggest the age group or demographic that these outfits would appeal to?"
]

import openai

openai.api_key = ''


def get_recommendation(image, queries_gpt):
    prompt = "You are an experianced fashion designer. Given the following images and queries, please provide a textual answers to the queries:\n as a very short description as a form of just key_words. not more than one keyword per each query, please be specific and pruduce descriptions that are different from photo to photo"

    prompt += f"![Image]({image})\n Queries {queries_gpt}\n"

    response = openai.Completion.create(
        engine='text-davinci-003',
        prompt=prompt,
        max_tokens=77,
        n=1,
        stop=None,
        temperature=0.7
    )

    recommendation = response.choices[0].text.strip()

    return recommendation

type(raw_image)

# Just needed in case you'd like to append it to an array
import os
image_paths_part2 = []
for filename in os.listdir('/content/drive/MyDrive/IndustrialML/train_data/'):
    if filename.endswith("jpg"):
        # Your code comes here such as
        image_paths_part2.append('/content/drive/MyDrive/IndustrialML/train_data/'+filename)

def get_gpt_descriptions(image_paths):
  descriptions = []
  for image in image_paths[:3]:
    raw_image = Image.open(image).convert('RGB')
    descriptions.append(get_recommendation(raw_image, queries_gpt))
  return descriptions

# descriptions = []

import time

for i in range(0, len(image_paths_part3)):
  print(image_paths_part3[i])
  raw_image = Image.open(image_paths_part3[i]).convert('RGB')
  success = 0
  while success != 1:
    try:
      descriptions_part3.append(get_recommendation(raw_image, queries_gpt))
      success = 1
    except:
      print("Waiting for 20 seconds.")
      time.sleep(30)
      print("Wait is over.")

# import pandas as pd

# dataset = pd.DataFrame({'image_path': image_paths, 'description': descriptionsa})

# dataset.to_csv('/content/drive/MyDrive/IndustrialML/train_data/dataset_with_short_descriptions_135 photos.csv')

# dataset.to_excel('/content/drive/MyDrive/IndustrialML/train_data/dataset_with_short_descriptions_135 photos.xlsx')